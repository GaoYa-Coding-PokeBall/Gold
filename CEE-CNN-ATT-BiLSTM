import torch
import torch.nn as nn
import pandas as pd
import numpy as np
import random
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt

data = pd.read_csv('C:\\Users\\win 10\\OneDrive\\Desktop\\dataset\\no_min_max_standardized_data_8.csv')
data = data.drop(columns=['date'])

closing_price = data.iloc[:, 0]
high = pd.read_csv('C:\\Users\\win 10\\OneDrive\\Desktop\\dataset\\high.csv', header=None)
low = pd.read_csv('C:\\Users\\win 10\\OneDrive\\Desktop\\dataset\\low.csv', header=None)
trend = pd.read_csv('C:\\Users\\win 10\\OneDrive\\Desktop\\dataset\\trend.csv', header=None)

data['a'] = trend
data['b'] = low

features = [col for col in data.columns if col not in ['a', 'b']]

random.seed(12345545796)
np.random.seed(123456)
torch.manual_seed(123456)
torch.cuda.manual_seed_all(123456)

scaler_features = MinMaxScaler(feature_range=(0, 1))
data_scaled_features = scaler_features.fit_transform(data[features])

scaler_a = MinMaxScaler(feature_range=(0, 1))
data_scaled_a = scaler_a.fit_transform(data[['a']])

scaler_b = MinMaxScaler(feature_range=(0, 1))
data_scaled_b = scaler_b.fit_transform(data[['b']])

data_scaled = np.hstack([data_scaled_features, data_scaled_a, data_scaled_b])

num_features = data_scaled_features.shape[1]

def create_sequences(data, window_size, num_features):
    sequences = []
    labels = []
    for i in range(len(data) - window_size):
        sequence = data[i:i + window_size, :num_features] 
        label = data[i + window_size, num_features:] 
        sequences.append(sequence)
        labels.append(label)
    return np.array(sequences), np.array(labels)

window_size = 30
X, y = create_sequences(data_scaled, window_size, num_features)

train_size = int(0.8 * len(X))
val_size = int(0.1 * len(X))
test_size = len(X) - train_size - val_size

X_train, X_val, X_test = X[:train_size], X[train_size:train_size + val_size], X[train_size + val_size:]
y_train, y_val, y_test = y[:train_size], y[train_size:train_size + val_size], y[train_size + val_size:]

class CNNBiLSTMModel(nn.Module):
    def __init__(self, input_size, cnn_channels, kernel_size, hidden_size, num_layers, output_size, num_heads):
        super(CNNBiLSTMModel, self).__init__()
        self.cnn = nn.Conv1d(in_channels=input_size, out_channels=cnn_channels, kernel_size=kernel_size)
        self.relu = nn.ReLU()
        self.pool = nn.MaxPool1d(kernel_size=4)

        self.lstm = nn.LSTM(input_size=cnn_channels, hidden_size=hidden_size, num_layers=num_layers,
                            batch_first=True, bidirectional=True)

        self.attention = nn.MultiheadAttention(embed_dim=hidden_size * 2, num_heads=num_heads, batch_first=True)

        self.fc = nn.Linear(hidden_size * 2, output_size)  

    def forward(self, x):
        # x: [batch_size, seq_length, input_size]
        x = x.permute(0, 2, 1)  
        x = self.cnn(x)  # [batch_size, cnn_channels, L_out]
        x = self.relu(x)
        x = self.pool(x)  # [batch_size, cnn_channels, L_out//4]
        x = x.permute(0, 2, 1) 

        h_0 = torch.zeros(self.lstm.num_layers * 2, x.size(0), self.lstm.hidden_size).to(x.device) 
        c_0 = torch.zeros(self.lstm.num_layers * 2, x.size(0), self.lstm.hidden_size).to(x.device)

        out, _ = self.lstm(x, (h_0, c_0)) 

        attn_output, attn_weights = self.attention(out, out, out)  

        context = torch.mean(attn_output, dim=1)  

        out = self.fc(context) 

        return out

# 模型参数
input_size = num_features 
cnn_channels = 25
kernel_size = 3 
hidden_size = 128  
num_layers = 3  
output_size = 2
num_heads = 8 

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = CNNBiLSTMModel(input_size, cnn_channels, kernel_size, hidden_size, num_layers, output_size, num_heads).to(device)

criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.0001,weight_decay=1e-8)

def to_tensor(data):
    return torch.tensor(data, dtype=torch.float32).to(device)

X_train_tensor = to_tensor(X_train)
y_train_tensor = to_tensor(y_train) 
X_val_tensor = to_tensor(X_val)
y_val_tensor = to_tensor(y_val)
X_test_tensor = to_tensor(X_test)
y_test_tensor = to_tensor(y_test)

train_rmse_list = []
val_rmse_list = []


num_epochs =300
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()

    outputs = model(X_train_tensor)
    loss = criterion(outputs, y_train_tensor)

    loss.backward()
    optimizer.step()

    train_rmse = np.sqrt(loss.item())
    train_rmse_list.append(train_rmse)

    model.eval()
    with torch.no_grad():
        val_outputs = model(X_val_tensor)
        val_loss = criterion(val_outputs, y_val_tensor)
        val_rmse = np.sqrt(val_loss.item())
        val_rmse_list.append(val_rmse)

    if (epoch + 1) % 10 == 0:
        print(f'Epoch [{epoch + 1}/{num_epochs}], Train RMSE: {train_rmse:.4f}, Val RMSE: {val_rmse:.4f}')

model.eval()
with torch.no_grad():
    y_pred_test = model(X_test_tensor).cpu().numpy()

y_pred_a = scaler_a.inverse_transform(y_pred_test[:, 0].reshape(-1, 1))
y_pred_b = scaler_b.inverse_transform(y_pred_test[:, 1].reshape(-1, 1))
y_pred_close = y_pred_a + y_pred_b  

y_test_a = scaler_a.inverse_transform(y_test[:, 0].reshape(-1, 1))
y_test_b = scaler_b.inverse_transform(y_test[:, 1].reshape(-1, 1))
y_test_close = y_test_a + y_test_b  


r2 = r2_score(y_test_close, y_pred_close)
rmse = np.sqrt(mean_squared_error(y_test_close, y_pred_close))
mae = mean_absolute_error(y_test_close, y_pred_close)
mape = np.mean(np.abs((y_test_close - y_pred_close) / y_test_close)) * 100

print(f'R² Score: {r2:.4f}')
print(f'RMSE: {rmse:.4f}')
print(f'MAE: {mae:.4f}')
print(f'MAPE: {mape:.4f}')


with torch.no_grad():
    y_pred_train = model(X_train_tensor).cpu().numpy()
    y_pred_val = model(X_val_tensor).cpu().numpy()


y_pred_train_a = scaler_a.inverse_transform(y_pred_train[:, 0].reshape(-1, 1))
y_pred_train_b = scaler_b.inverse_transform(y_pred_train[:, 1].reshape(-1, 1))
y_pred_train_close = y_pred_train_a + y_pred_train_b


y_pred_val_a = scaler_a.inverse_transform(y_pred_val[:, 0].reshape(-1, 1))
y_pred_val_b = scaler_b.inverse_transform(y_pred_val[:, 1].reshape(-1, 1))
y_pred_val_close = y_pred_val_a + y_pred_val_b


plt.figure(figsize=(14, 7))


actual_close_price = closing_price.values


train_predict_plot = np.empty_like(actual_close_price)
train_predict_plot[:] = np.nan
train_predict_plot[window_size:train_size + window_size] = y_pred_train_close.flatten()

val_predict_plot = np.empty_like(actual_close_price)
val_predict_plot[:] = np.nan
val_predict_plot[train_size + window_size:train_size + val_size + window_size] = y_pred_val_close.flatten()

test_predict_plot = np.empty_like(actual_close_price)
test_predict_plot[:] = np.nan
test_predict_plot[train_size + val_size + window_size:] = y_pred_close.flatten()


plt.plot(actual_close_price, label="Actual Value", color='blue')

# Plot the training set prediction
plt.plot(train_predict_plot, label="Training Set Prediction", color='green')

# Plot the validation set prediction
plt.plot(val_predict_plot, label="Validation Set Prediction", color='orange')

# Plot the test set prediction
plt.plot(test_predict_plot, label="Test Set Prediction", color='red')

# Add vertical dashed lines to separate the training, validation, and test sets
plt.axvline(x=train_size + window_size, color='grey', linestyle='--', label='Training/Validation and Test Divider')
plt.axvline(x=train_size + val_size + window_size, color='grey', linestyle='--')

plt.title("CNN+BiLSTM+Attention+EMD")
plt.xlabel("Time Step")
plt.ylabel("Closing Price")
plt.legend()
plt.show()

# ======================
# Plot the change curve of RMSE
plt.figure(figsize=(12, 6))
plt.plot(train_rmse_list, label="Training Set RMSE")
plt.plot(val_rmse_list, label="Validation Set RMSE")
plt.title("Change of RMSE during Training")
plt.xlabel("Epoch")
plt.ylabel("RMSE")
plt.legend()
plt.show()
